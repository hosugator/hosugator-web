---
created: 2025-10-12
tags:
  - nlp
  - dnn
  - cnn
  - dl
  - ml
reference:
---
> **Last revised:** `= dateformat(this.file.mtime, "yyyy-MM-dd HH:mm")`

# DLL 챗봇
## tf library: tokenizer
tensorflow의 tokenizer를 이용하면, 이전에 진행했던 토큰화 작업을 tfidf를 사용하는 것보다 간소화가 가능하다.
정수 인코딩된 X_train을 모델 학습할 때는 실수의 형태로 정규화/표준화하여 진행하게 된다.
다만 모델 학습 시에만 Embedding 층에서 자동 변환하고 다시 정수 형태로 반환한다.
이는 실수의 형태가 학습에는 유리하지만, 저장 시에는 정수의 형태가 효율적이기 때문이다.
이 때 y_pred는 실수의 형태로 반환되기에, sigmoid 함수를 이용해 0과 1로 만들어 정답과 비교한다.
최종 결과는 classification_report를 이용하여, 최종 결과 지표들을 일괄 확인할 수 있다.
확인해보면 정확도가 약 0.78로 나오는데, 이전에 생성한 ML 모델과 큰 차이가 없다.
결과적으로 문장 데이터도 ML/DL 간 큰 차이가 없으며, DL이 충분한 성능을 발휘하기 위해서는 방대한 데이터와 이를 사용 가능한 고수준의 하드웨어가 필요하다.
## DL은 결국 행렬 연산이다
입력층에 매개변수가 곱해져 다음 층의 입력값이 되는데, 이는 결국 행렬의 연산이다.
예를 들어 입력층이 (3,1)이고 다음 층이 (2,1)이면, 매개변수는 (2, 3)이 된다.
다만 단순 곱이 아니라 곱을 거친 후 활성화 함수를 곱하고 다시 bias를 더해서 다음 층의 노드가 된다.
하지만 입력층의 데이터가 거대한 경우에는 batch가 필요하기에 사실상 (3, 1)이 아닌 (3^x, 1)이 된다.
다음 층의 노드도 마찬가지로 (2, 1)이 아닌 (2^x, 1)이 된다.
결과적으로 하나의 데이터 뭉치가 아닌 배치로 나뉜 데이터 리스트라고 보면 된다.
### 딥러닝 행렬 연산의 기본
딥러닝의 각 층은 다음의 연산을 거칩니다:
다음층의입력=(현재층의출력×가중치행렬)+편향벡터
이것을 행렬 연산으로 표현하면 다음과 같습니다.
- **입력 데이터**: X (크기: nsamples×nfeatures)
- **가중치 행렬**: W (크기: nfeatures×nneurons)
- **편향 벡터**: b (크기: 1×nneurons)
- **다음 층의 입력**: A (크기: nsamples×nneurons)
A=XW+b
이후 활성화 함수(σ)를 적용하여 최종 출력을 얻습니다.
다음층의출력=σ(A)=σ(XW+b)
### 배치 연산의 이해
말씀하신 대로, 딥러닝은 한 번에 하나의 데이터가 아닌 **배치(batch)** 단위로 연산합니다. 이렇게 하는 이유는 다음과 같습니다.
- **병렬 처리**: GPU는 행렬 연산에 최적화되어 있어, 여러 데이터를 한 번에 처리하는 것이 효율적입니다.
- **학습 안정화**: 한 번에 너무 많은 데이터를 사용하면 메모리 문제가 발생할 수 있고, 너무 적은 데이터를 사용하면 학습이 불안정해질 수 있습니다. 배치는 이 두 가지 문제의 균형을 잡아줍니다.
따라서 입력 데이터 X의 크기는 **`(배치 크기, 입력 특징 수)`**가 됩니다.
예시를 들어 설명해 보겠습니다.
- **입력층**: `(배치 크기, 3)`
- **다음 층**: `(배치 크기, 2)`
- **가중치 행렬**: `(3, 2)`
연산은 다음과 같습니다:
입력 행렬 (배치 크기, 3) X 가중치 행렬 (3, 2) = 결과 행렬 (배치 크기, 2)
결론적으로, **딥러닝의 행렬 연산은 배치 개념을 포함하여 `(배치 크기, 특징 수)` 형태의 행렬이 가중치 행렬과 곱해지는 방식으로 진행됩니다.**
이는 한 번의 연산으로 배치 내 모든 데이터의 다음 층 출력을 계산하여 효율을 극대화합니다.
# RNN/LSTM/GRN
## 맥락을 이해하는 알고리즘, RNN
rnn의 연산은 dnn과는 다소 다른데, time step을 가진다.
다시 말해, 다음 층의 입력(노드)은 이전 층의 입출력의 영향을 받으며, tanh를 활성화 함수로 사용한다.
그러므로, 과거 정보를 기억하고 맥락을 파악할 수 있다.
다만, 장기 의존성 문제가 발생했는데, 초기 정보가 후단으로 갈수록 희석된다.
이를 방지하기 위해, 초기 정보를 따로 저장하는 LSTM/GRN 등이 등장했다.
현재는 rnn 역시 사용하지 않고 있으며, 날씨, 주식과 같은 시계열 데이터에서만 간혹 사용된다.
## 지난 알고리즘을 배우는 의의
현재 쓰이지 않는 전통적인 알고리즘을 배우는 이유는, 그것이 기초에서부터 난제까지의 자연스러운 발전 흐름이기 때문이다.
흐름을 따라가다보면, 어떤 로직이 어떤 기능/한계점을 지니는지를 파악 가능하다.
해당 알고리즘들을 직접 손코딩으로 짜는 능력을 기르는 게 아니다.
내가 원하는 결과를 얻기 위해서, 데이터를 어떤 로직을 거쳐야 하는지를 이해하면 된다.
예를 들어, 배운 알고리즘의 기능들을 조합하여, 스팸 메일 차단, 고객 후기 관리 등의 프로그램을 짤 수 있다.
# 일지
## 사실
딥러닝 모델 학습은 행렬 연산을 기반으로 한다.
tf.keras.Tokenizer는 텍스트를 정수 인코딩하는 작업을 간소화한다.
학습 과정에서 Embedding 층이 정수 데이터를 실수 벡터로 변환해 모델 학습에 사용하고, 저장 시에는 효율적인 정수 형태로 되돌린다.
y_pred는 실수 형태로 나오기 때문에 0.5를 기준으로 1 또는 0으로 변환한 후, accuracy_score를 사용해 정확도를 평가한다.
RNN은 시계열 데이터의 맥락을 이해하는 데 유용하지만, 장기 의존성 문제 때문에 LSTM/GRU 같은 발전된 모델이 등장했다.
전통적인 알고리즘을 배우는 이유는 기술의 발전 흐름과 각 로직의 한계 및 기능을 이해하기 위해서이다.
## 교훈
오늘 배운 딥러닝 개념들은 단순히 코드를 짜는 것뿐만 아니라, 데이터가 모델을 통해 어떻게 변환되고 연산되는지 그 원리를 이해하는 데 도움이 되었다.
특히 배치 연산과 행렬 곱셈에 대한 설명이 딥러닝의 효율성을 왜 GPU를 사용하는지 명확하게 이해하게 해 주었다.
RNN의 장기 의존성 문제와 LSTM의 등장 배경을 배우면서 딥러닝 기술이 어떻게 발전해 왔는지 흥미롭게 느꼈다.
## 행동
앞으로 모델을 구축할 때 Embedding 층의 입력 및 출력 형태를 더 주의 깊게 설정할 것이다.
실제 프로젝트에 적용할 때, 데이터의 양과 모델의 성능 관계를 염두에 두고, 데이터가 충분하지 않다면 기존의 ML 모델을 사용하는 것도 고려할 것이다.
NumPy를 활용한 원-핫 인코딩 로직을 복습하여 딥러닝 내부 연산에 대한 이해를 더 깊게 할 것이다.