## 사실 (Facts)
NLP 알고리즘 변천사 (DNN → RNN → LSTM → GRU → Seq2seq → Attention → Transformer)
### **주요 학습 내용**
- **DNN:** 순서 없는 데이터에 적합하며, 순차 데이터 처리에 한계가 있음.
- **RNN/LSTM/GRU:** 순차 데이터 처리를 위해 고안된 모델들로, RNN은 기울기 소실 문제, LSTM은 게이트를 통해 장기 의존성 문제 해결, GRU는 LSTM의 단순화 버전이다.
- **Seq2seq:** 인코더-디코더 구조로, 긴 문장을 단일 벡터로 압축하는 '정보 병목' 현상이라는 한계를 가지고 있음.
- **Attention:** Seq2seq의 정보 병목 현상을 해결하기 위해 등장. 인코더의 모든 은닉 상태에 접근하여 관련성 높은 정보에 동적으로 '집중'함으로써 성능을 향상시켰다. 하지만 RNN 기반이기에 완전한 병렬 처리가 불가능하다는 한계가 있다.
- **Transformer:** RNN 구조를 완전히 버리고 오직 **셀프 어텐션(Self-Attention)**만으로 병렬 처리를 가능하게 한 혁신적인 모델이다. 이로 인해 학습 속도가 매우 빠르고, 모든 시퀀스가 동일한 질의 정보를 갖게 된다.
- **모델 평가 지표:** 텍스트 요약 모델의 성능을 평가하기 위해 **ROUGE(Recall-Oriented Understudy for Gisting Evaluation)**라는 지표를 사용하며, ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S 등이 있다.
- **실용적인 접근:** 대규모 데이터와 계산 자원이 필요한 트랜스포머는 직접 학습하기보다, **Pre-trained 모델(사전 학습 모델)**을 활용한 전이 학습(Transfer Learning)을 통해 성능을 얻는 것이 일반적이다.
- **배치 처리:** 모든 딥러닝 모델은 가변 길이의 문장을 처리하기 위해 **패딩(Padding)**을 통해 입력 형태를 고정한다.
## 느낌 (Feelings)
**NLP 알고리즘의 발전 과정이 매우 논리적이고 단계적으로 이해되었다.** 특히, RNN의 한계(기울기 소실, 병렬 처리 불가)가 LSTM과 어텐션, 최종적으로 트랜스포머가 등장하게 된 이유를 명확히 알게 되어 좋았다.
**이론적인 내용과 실제 코드를 함께 보니 개념이 더 명확해졌다.** 특히 트랜스포머의 `MultiHeadSelfAttention` 레이어 코드를 통해 `Query`, `Key`, `Value`의 역할과 내적 연산 과정이 어떻게 구현되는지 구체적으로 알 수 있었다.
## 행동 (Actions)
오늘 배운 내용을 바탕으로 **팀 프로젝트를 구체화할 계획이다.** 한글 추론 모델 생성을 목표로, 데이터 수집부터 전처리, 그리고 모델 선택과 평가에 이르기까지 오늘 학습한 내용을 적극적으로 활용할 것이다.
**ROUGE 지표**를 팀 프로젝트의 주요 평가 기준으로 삼고, ROUGE-1, ROUGE-2, ROUGE-L 등의 점수를 통해 모델의 성능을 객관적으로 분석할 것이다.
**Hugging Face의 사전 학습된 한글 트랜스포머 모델**을 조사하고, 팀 프로젝트에 적용할 수 있는 방법을 구체적으로 논의할 것이다. 직접 모델을 처음부터 학습시키는 대신, 전이 학습을 통해 효율적으로 프로젝트를 진행할 것이다.

#nlp #dnn #rnn #lstm #gru #seq2seq #attention #transformer 