#transformer #rnn #lstm #attention #encoder #decoder #seq2seq #hidden-layer #context-vector #weight #bptt #parallel-processing
이 질의응답은 RNN과 LSTM이 문장 전체를 하나의 고정된 벡터로 요약하는 이유와 그 한계에 대한 의문에서 시작되었다.
이로부터 어텐션과 트랜스포머 모델이 등장하게 된 배경과 그 작동 원리에 대한 깊이 있는 탐구가 이어졌다.
질문의 핵심인 어텐션의 구조적 한계와 트랜스포머의 질적 균등화 개념을 추가하여 내용을 완성한다.
### RNN의 순방향 연산과 정보 병목 현상
RNN과 LSTM은 문장의 첫 단어부터 마지막 단어까지 순서대로 읽어나가며 각 시점의 은닉 상태 벡터를 생성한다.
각 은닉 상태는 이전 시점까지의 정보만 포함하므로, 문장 초반부의 벡터는 필연적으로 후반부 단어의 문맥을 알지 못한다.
초기의 Seq2seq 모델은 인코더의 마지막 은닉 상태만 디코더에게 전달했다.
이 하나의 고정된 벡터가 전체 문장의 정보를 압축해야 했기 때문에, 문장이 길어질수록 중요한 정보가 손실되는 정보 병목 문제가 발생했다.
### 어텐션의 등장: 정보 병목 해소
어텐션은 이 정보 병목 현상을 해결하기 위해 등장했다.
어텐션은 인코더가 생성한 모든 시점의 은닉 상태 벡터를 디코더에게 전달한다.
디코더는 출력 단어를 하나씩 예측할 때마다 인코더의 모든 은닉 상태 벡터에 가중치(attention score)를 부여하고, 이를 기반으로 매 순간 새로운 문맥 벡터를 생성한다.
이 과정은 학습이 아닌 참조에 가까우며, 문장 길이에 관계없이 필요한 정보에만 집중하므로 예측 성능을 유지한다.
### 트랜스포머의 혁신: 질적 균등화
어텐션이 해결하지 못한 근본적인 문제는 RNN 인코더의 순차적 처리 한계였다.
순방향 연산에서는 여전히 초기 은닉 상태가 후기 은닉 상태보다 정보의 질이 낮았다.
트랜스포머는 이러한 문제를 해결하기 위해 순차적 연산을 완전히 제거했다.
핵심인 셀프 어텐션은 모든 입력 시퀀스를 한 번에 병렬적으로 처리하여, 각 시퀀스가 문장 내의 모든 다른 시퀀스와 관계를 계산하도록 한다.
이로 인해 모든 입력 시퀀스들은 문장 전체의 문맥을 완벽하게 반영한, 동일한 질의 은닉 상태 벡터를 생성하게 된다.
### 학습 과정의 핵심: 가중치와 역전파
가중치는 모델이 학습하는 '전략'에 해당하는 고정된 파라미터이고, 은닉 상태 벡터는 입력에 따라 매번 생성되는 '내용'이다.
RNN 모델은 모든 시점에서 동일한 가중치를 공유하며, BPTT(시간에 따른 역전파)를 통해 가중치를 업데이트한다.
역전파가 진행되더라도 순방향 연산의 결과(은닉 상태)는 바뀌지 않는다.
역전파는 다음 순방향 연산 시 가중치 계산을 최적화할 뿐이다.
### 결론
RNN 기반 어텐션 모델은 순차적 처리로 인해 필연적으로 발생하는 초기 은닉 상태의 정보 부족을 어텐션 메커니즘을 통해 우회적으로 해결했다.
반면 트랜스포머는 모든 입력 시퀀스가 동일한 질의 정보를 포함하도록 구조적으로 설계함으로써 이 문제를 근본적으로 해결하고, 병렬 처리의 이점까지 얻게 되었다.