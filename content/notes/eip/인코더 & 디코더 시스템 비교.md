Transformer가 등장하기 전까지 인코더-디코더 시스템은 주로 RNN(LSTM, GRU) 기반으로 구축되었다.
인코더와 디코더의 역할은 입력 시퀀스를 받아 출력 시퀀스를 생성하는 것이다. 세 가지 모델은 이 과정에서 정보를 주고받는 방식에 차이가 있다.
아래는 인코더-디코더 시스템 관점에서 Transformer의 차이점을 이전 알고리즘과 비교한 내용이다.
RNN 기반 모델에서는 '시간'이 흐름에 따라 정보가 누적되는 방식이라면, Transformer는 '시간'의 개념 없이 모든 단어 간의 관계를 한꺼번에 파악한다.
그렇기에 RNN 기반에서는 초기 단어의 벡터는 적은 정보를 가지고 있는 한계가 있는데 반해, Transformer는 각 단어가 전체 입력 시퀀스의 맥락을 이해한다.
이에 자기 자신과의 관계마저도 계산한다는 의미에서 ‘셀프 어텐션’이라고 표현합니다.
## Seq2seq (RNN 기반)
Seq2seq은 인코더와 디코더를 연결하는 가장 기본적인 형태다.
### 인코더
- **입력값**: 단어들의 시퀀스(`w1`, `w2`, `w3`, ...).
- **출력값**: 입력 시퀀스 전체를 요약한 **고정된 크기의 벡터 하나**. 이 벡터는 문맥 벡터(Context Vector)라고 불리며, 디코더의 초기 은닉 상태로 사용된다.
### 디코더
- **입력값**: 인코더의 **최종 문맥 벡터**와 이전 단계의 출력 단어.
- **출력값**: 번역된 단어들의 시퀀스(`v1`, `v2`, `v3`, ...).
## Attention (RNN 기반)
어텐션은 Seq2seq의 한계(긴 문장에서의 정보 손실)를 보완하기 위해 도입되었다. 인코더가 모든 시점의 정보를 디코더에 제공하여 디코더가 필요한 정보에만 '집중'할 수 있게 한다.
### 인코더
- **입력값**: 단어들의 시퀀스(`w1`, `w2`, `w3`, ...).
- **출력값**: **모든 타임스텝의 은닉 상태(Hidden State)** 시퀀스.
### **디코더**
- **입력값**: 이전 단계의 출력 단어, 그리고 인코더에서 받은 **모든 은닉 상태 벡터**들. 디코더는 현재 예측에 가장 중요한 은닉 상태에 어텐션 가중치를 부여한다.
- **출력값**: 번역된 단어들의 시퀀스(`v1`, `v2`, `v3`, ...).
## Transformer (Attention 기반)
Transformer는 RNN의 순차적 계산을 완전히 제거하고, 오직 어텐션만으로 인코더와 디코더를 구성한다. 이는 병렬 처리를 가능하게 해 학습 속도를 획기적으로 높였다.
### **인코더**
- **입력값**: 단어들의 시퀀스(`w1`, `w2`, `w3`, ...).
- **출력값**: **각 단어에 대한 문맥 정보를 담은 벡터 시퀀스**. 셀프 어텐션 메커니즘을 통해 각 단어는 문장 내의 모든 다른 단어와 관계를 계산하여 문맥을 파악한다.
### **디코더**
- **입력값**: 인코더의 모든 출력 벡터와, 지금까지 생성된 출력 시퀀스.
- **출력값**: 번역된 단어들의 시퀀스(`v1`, `v2`, `v3`, ...).
## 요약 비교표
| 모델 | 인코더 출력값 | 디코더 입력값 |
| --------------- | ------------------- | ------------------------- |
| **Seq2seq** | 최종 시점의 **단일 문맥 벡터** | 인코더의 단일 문맥 벡터 + 이전 출력 단어 |
| **Attention** | **모든 시점의 은닉 상태** | 모든 은닉 상태 + 이전 출력 단어 |
| **Transformer** | **각 단어의 문맥 벡터** | 인코더의 모든 문맥 벡터 + 이전 출력 시퀀스 |
#transformer #attention #nlp #seq2seq #self_attention #encoder #decoder 