---
created: 2025-10-12
tags: [kdlc, hyperparameter_tuning, grid_search, random_search, time_series_cross_validation, xgboost, lightgbm, lstm]
reference:
  - "[[kdlc - hub]]"
---
> **Last revised:** `= dateformat(this.file.mtime, "yyyy-MM-dd HH:mm")`


# 하이퍼파라미터 튜닝 전략

## 핵심 용어 정의
### 하이퍼파라미터 튜닝 핵심 용어 정의표
| 용어 | 정의 |
| :--- | :--- |
| 하이퍼파라미터 튜닝 | 모델의 성능을 최적화하기 위해 학습 전에 사람이 직접 설정하는 값(예: 학습률, 깊이)들의 최적 조합을 찾는 과정이다. |
| Grid Search (그리드 탐색) | 탐색 공간 내의 모든 가능한 하이퍼파라미터 조합을 시도하여 최적의 조합을 찾는 방법이다. 시간이 오래 걸리지만 최적 해를 찾을 가능성이 높다. |
| Random Search (랜덤 탐색) | 탐색 공간 내에서 무작위로 하이퍼파라미터 조합을 샘플링하여 시도하는 방법이다. 그리드 탐색보다 빠르면서도 효율적인 탐색이 가능하다. |
| TimeSeriesSplit | 시계열 데이터의 시간 순서를 유지하면서 교차 검증(Cross-validation)을 수행하는 방법이다. 학습 데이터가 검증 데이터보다 항상 과거 시점에 위치하도록 한다. |
| learning_rate (학습률) | 모델이 손실 함수를 최적화할 때 한 번에 움직이는 보폭을 결정하는 파라미터이다. 너무 크면 불안정하고, 너무 작으면 학습 속도가 느려진다. |
| max_depth (최대 깊이) | 트리 계열 모델에서 트리가 분기할 수 있는 최대 깊이이며, 모델의 복잡도와 과적합을 제어한다. |
| units (유닛/노드 수) | LSTM 레이어 내에 존재하는 메모리 셀의 개수이며, 모델이 학습할 수 있는 정보량을 결정한다. |

## 1. 하이퍼파라미터 튜닝 방법론
모델의 예측 성능을 극대화하기 위해 Grid Search와 Random Search를 조합하여 효율적으로 최적의 하이퍼파라미터 조합을 탐색한다.

* **초기 탐색 (Random Search):** 광범위한 하이퍼파라미터 공간에서 랜덤 샘플링을 통해 빠르게 유망한 영역을 식별한다.
* **최적화 (Grid Search):** Random Search로 식별된 유망 영역 주변에서 좁은 범위의 하이퍼파라미터 조합을 집중적으로 탐색하여 최종 최적값을 확정한다.

## 2. 시계열 교차 검증 (TimeSeriesSplit) 적용
시계열 데이터의 특성상 일반적인 K-Fold 교차 검증은 데이터 누수(Data Leakage)를 유발할 수 있다. 따라서 TimeSeriesSplit을 사용하여 교차 검증을 수행한다.

* **원칙:** TimeSeriesSplit은 학습 데이터가 항상 검증 데이터보다 시간적으로 앞서도록 분할을 생성한다. 이는 모델이 미래를 예측하는 실제 상황을 가장 잘 모방한다.
* **적용:** 튜닝 과정에서 생성되는 모든 하이퍼파라미터 조합에 대해 TimeSeriesSplit을 적용하여 검증 성능을 측정한다.

## 3. 모델별 주요 튜닝 파라미터 예시
각 모델의 핵심 성능 및 복잡도 제어에 영향을 미치는 주요 파라미터를 중심으로 튜닝을 진행한다.

### 3.1. 트리 계열 모델 (XGBoost, LightGBM)
* **learning\_rate:** 모델의 학습 속도를 조절하는 핵심 파라미터이다.
* **max\_depth:** 트리의 최대 깊이를 조절하여 모델의 복잡도와 과적합을 제어한다.
* **n\_estimators:** 생성할 트리의 개수이다. 충분히 큰 값으로 설정하고 Early Stopping과 함께 사용하는 것이 일반적이다.
* **subsample, colsample\_bytree:** 데이터를 샘플링하여 학습에 사용함으로써 과적합을 방지하고 일반화 성능을 향상시킨다.

### 3.2. 딥러닝 모델 (LSTM)
* **units:** LSTM 레이어의 노드 수를 조절하여 모델의 학습 용량을 제어한다.
* **dropout:** 과적합을 방지하기 위해 학습 중 무작위로 뉴런을 비활성화하는 비율을 설정한다.
* **batch\_size:** 한 번의 가중치 업데이트에 사용되는 데이터 샘플의 크기를 결정한다.
* **epoch:** 전체 학습 데이터셋을 반복하여 학습하는 횟수이다. Early Stopping을 통해 최적 시점에서 학습을 중단한다.
