#rag #llm #hallucination #context #db 
## RAG이란
RAG 시스템은 LLM의 지식 부족 문제를 해결하고, 최신 정보와 정확한 출처를 제공하기 위한 기술이다. 
검색(Retrieval)과 생성(Generation)을 결합한 LLM의 새로운 아키텍처이다.
- 검색(Retrieval): 사용자 질문이 들어오면, 먼저 벡터화된 외부 지식 데이터베이스에서 질문과 가장 관련성이 높은 문서를 검색
- 증강(Augmented): 검색된 문서를 LLM에 입력할 프롬프트에 추가하여 LLM의 지식을 보강
- 생성(Generation): 보강된 프롬프트를 바탕으로 LLM이 답변을 생성
### 강점
- 환각 감소: LLM은 학습하지 않은 정보에 대해 그럴듯한 거짓 정보를 만들어낼 수 있으나, RAG는 외부 문서를 제공함으로써 이 문제를 해결하고 정확성을 높임
- 최신 정보: LLM은 특정 시점까지의 데이터만 학습하므로 최신 정보를 알지 못하나, RAG는 실시간으로 업데이트되는 데이터베이스를 통해 항상 최신 정보를 활용
- 출처 제공: 답변의 근거가 되는 출처 문서를 함께 제시하여 신뢰성 확보
### 사용
- 내부 문서 활용: 기업에서도 로컬화하여, 내부 문서들을 기반으로 LLM을 사용 가능
- 보고서: 여러 LLM을 바탕으로, RAG 시스템을 적용하여 많은 자료들을 바탕으로 한 통합 보고서를 단시간에 제작
## 원리
RAG 시스템의 핵심은 내부적으로 DB를 구축하고, 질문과 자료를 벡터화하여 LLM과 연동하는 과정에 있다.
### 단순 웹 검색의 한계
웹 브라우저에 여러 자료를 올려놓고 LLM에 요청하는 방식은 LLM의 컨텍스트 창(Context Window) 한계로 인해 제대로 작동 불가
컨텍스트 창은 LLM이 한 번에 처리할 수 있는 정보의 양이므로, 많은 양의 자료를 모두 담을 수 없다.
게다가 이 방식은 비효율적이며, LLM이 자료의 특정 부분을 놓칠 위험성 존재
### RAG의 차별점
이 방식은 DB에 자료를 미리 임베딩(벡터화)해놓고, 사용자 질문이 들어오면 LLM API에 모든 자료를 보내는 것이 아니다.
질문과 가장 관련성이 높은 몇 개의 자료 조각(청크)만 검색해서 함께 보내준다.
이 덕분에 컨텍스트 창의 제약을 효과적으로 극복한다.
### RAG 시스템 구축 과정의 핵심
#### 지식 저장소 구축:
 PDF, 문서, 웹 페이지 등 보고서에 필요한 모든 자료를 수집
 이를 텍스트 청크로 분할하고, 임베딩 모델을 사용하여 각 청크를 의미 있는 벡터로 변환
 변환된 벡터들은 벡터 데이터베이스(Vector Database)에 저장되며, 일반적인 DB와 달리 벡터 간의 유사도를 빠르게 찾아주는 데 최적화
#### 질문 처리 및 검색:
 사용자가 LLM에게 보고서 작성을 요청하면, 질문 또한 임베딩 모델을 통해 벡터로 변환
 이 벡터를 사용하여 벡터 DB에서 가장 유사한 벡터(즉, 질문과 가장 관련성이 높은 자료 청크)를 검색
#### 프롬프트 증강:
 검색된 자료 청크들을 LLM의 프롬프트에 추가
 예를 들어, "다음 자료들을 참고하여 보고서를 작성해 줘. [자료 A], [자료 B], [자료 C]..."와 같은 방식으로 프롬프트를 구성
#### LLM 답변 생성:
 LLM은 증강된 프롬프트를 바탕으로 답변을 생성
 이때 LLM은 자체 지식뿐만 아니라 제공된 외부 자료를 활용하여 더 정확하고 최신 정보를 반영한 보고서 초안 생성
## 결론
RAG 시스템은 LLM의 한계를 보완하고, 외부 자료를 효율적으로 활용하기 위한 별도의 데이터 처리 및 관리 과정이 필수적이다.
단순히 API를 호출하는 것을 넘어, 벡터 DB와 임베딩 모델을 포함한 전체 파이프라인을 구축해야 한다.